mlflow:
  experiment_name: str()

ray:
  num_cpus: int(min=1)

env: str()  # Add this to match the `env` field in config.yaml

unity_env:
  worker_id: int()
  base_port: int(min=1024, max=65535)
  no_graphics: bool()
  seed: int()
  log_folder: str()
  timeout_wait: int(min=1)  # Added to match timeout_wait in config.yaml

env_config:
  initial_agent_count: int(min=1)
  episode_horizon: int(min=1)
  rewards: map()  # Add to match the rewards dictionary in config.yaml

ppo_config:
  framework: enum('torch', 'tf', 'tf2')
  num_gpus: int(min=0)

multi_agent:
  policies:
    shared_policy:
      observation_space:
        type: str()  # e.g., "Box", "Discrete"
        shape: list()  # Dimensions of observation space
      action_space:
        type: str()  # e.g., "Box", "Discrete", "Hybrid"
        shape:
          discrete: list()  # Dimensions of discrete action space (if type is "Hybrid")
          continuous: list()  # Dimensions of continuous action space (if type is "Hybrid")

rollouts:
  num_env_runners: int(min=0)  # Added to match num_env_runners in config.yaml
  num_envs_per_env_runner: int(min=1)  # Added to match num_envs_per_env_runner in config.yaml
  rollout_fragment_length: any()  # Allow "auto" or an int value
  batch_mode: enum('truncate_episodes', 'complete_episodes')

training:
  train_batch_size: int(min=1)
  sgd_minibatch_size: int(min=1)
  num_sgd_iter: int(min=1)
  lr: num(min=0)
  gamma: num(min=0, max=1)
  lambda: num(min=0, max=1)
  clip_param: num(min=0)
  vf_clip_param: num(min=0)
  entropy_coeff: num(min=0)
  kl_coeff: num(min=0)
  vf_loss_coeff: num(min=0)

environment:
  disable_env_checking: bool()

stop:
  training_iteration: int(min=1)
