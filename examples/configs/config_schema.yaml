mlflow:
  experiment_name: str()

ray_init:
  num_cpus: int(min=1)
  num_gpus: int(min=0)

env: str()  # Add this to match the `env` field in config.yaml

unity_env:
  worker_id: int()
  base_port: int(min=1024, max=65535)
  no_graphics: bool()
  seed: int()
  log_folder: str()
  timeout_wait: int(min=1)  # Added to match timeout_wait in config.yaml

env_config:
  initial_agent_count: int(min=1)
  episode_horizon: int(min=1)

  rewards:
    off_road_penalty: num()
    on_road_reward: num()
    collision_with_other_agent_penalty: num()
    median_crossing_penalty: num()

ppo_config:
  framework: enum('torch', 'tf', 'tf2')
  num_gpus: int(min=0)

multi_agent:
  policies:
    shared_policy:
      observation_space:
        type: str()  # e.g., "Box", "Discrete"
        shape: list()  # Dimensions of observation space
      action_space:
        type: str()  # e.g., "Box", "Discrete", "Tuple"
        shape:
          discrete: list()  # Dimensions of discrete action space (if type is "Tuple")
          continuous: list()  # Dimensions of continuous action space (if type is "Tuple")

env_runners:
  num_env_runners: int(min=1)
  num_envs_per_env_runner: int(min=1)
  num_cpus_per_env_runner: int(min=1)
  num_gpus_per_env_runner: int(min=0)
  rollout_fragment_length: str()
  batch_mode: enum('truncate_episodes', 'complete_episodes')

training:
  num_workers: int(min=1)
  num_gpus: int(min=0)
  train_batch_size: int(min=1)
  sgd_minibatch_size: int(min=1)
  num_epochs: int(min=1)
  lr: num(min=0)
  gamma: num(min=0, max=1)
  lambda: num(min=0, max=1)
  clip_param: num(min=0)
  vf_clip_param: num(min=0)
  entropy_coeff: num(min=0)
  kl_coeff: num(min=0)
  vf_loss_coeff: num(min=0)

environment:
  disable_env_checking: bool()

stop:
  training_iteration: int(min=1)