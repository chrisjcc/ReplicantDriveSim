mlflow:
  experiment_name: "MARLExperiment"

ray_init:
  num_cpus: 4  # Total available CPUs across the entire setup
  num_gpus: 0

unity_env:
  worker_id: 0
  base_port: 5004
  no_graphics: false
  seed: 42
  log_folder: ./Logs

env_config:
  initial_agent_count: 1
  episode_horizon: 1000

  # Reward configuration
  rewards:
    off_road_penalty: -0.5
    on_road_reward: 0.01
    collision_with_other_agent_penalty: -1.0
    median_crossing_penalty: -1.0

ppo_config:
  framework: "torch"
  num_gpus: 0

multi_agent:
  policies:
    shared_policy:
      observation_space:
        type: "Box"  # Continuous observation space
        shape: [68]  # 68 continuous values in the observation space
      action_space:
        type: "Tuple"  # Custom type to indicate a combination of discrete and continuous spaces
        shape:
          discrete: [5]  # 5 discrete actions
          continuous: [3]  # 3 continuous actions

env_runners:
  num_env_runners: 1                # Use one environment runner
  num_envs_per_env_runner: 1        # One environment per runner (as you're running a single environment)
  num_cpus_per_env_runner: 1        # Assign one CPU per environment runner (you have only 4 CPUs available)
  num_gpus_per_env_runner: 0        # No GPUs per environment runner
  rollout_fragment_length: "auto"
  batch_mode: "truncate_episodes"

training:
  num_workers: 3                    # Allocate 2 worker processes for training (this is feasible with 4 CPUs)
  num_gpus: 0                       # No GPUs for training
  train_batch_size: 4096
  sgd_minibatch_size: 256           # Consider values in the range of 128â€“256
  num_epochs: 5                     # Training for only one epoch is too low for PPO, increasing it convergence
  lr: 3.0e-4
  gamma: 0.99
  lambda: 0.95
  clip_param: 0.2
  vf_clip_param: 1.0                # Lowering it to 1.0 or 2.0 provides better value function stability
  entropy_coeff: 0.01
  kl_coeff: 0.2                     # PPO relies on KL divergence to control policy updates, a 0.5 coefficient may be too aggressive, 0.2 is a more standard starting point
  vf_loss_coeff: 0.25               # If the value function dominates training loss, reducing this coefficient helps balance policy and value learning, consider 0.5 or even 0.25
  checkpoint_at_end: true
  checkpoint_freq: 1
  num_to_keep: 3

environment:
  disable_env_checking: false

stop:
  training_iteration: 5
