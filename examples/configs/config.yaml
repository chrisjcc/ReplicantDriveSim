mlflow:
  experiment_name: "MARLExperiment"

ray_init:
  num_cpus: 4  # Total available CPUs across the entire setup
  num_gpus: 0

unity_env:
  worker_id: 0
  base_port: 5004
  no_graphics: false
  seed: 42
  log_folder: ./Logs

env_config:
  initial_agent_count: 1
  episode_horizon: 1000

  # Reward configuration
  rewards:
    off_road_penalty: -0.5
    on_road_reward: 0.01
    collision_with_other_agent_penalty: -1.0
    median_crossing_penalty: -1.0

ppo_config:
  framework: "torch"
  num_cpus_per_worker: 2
  num_gpus: 0

multi_agent:
  policies:
    shared_policy:
      observation_space:
        type: "Box"  # Continuous observation space
        shape: [5]  # 5 continuous values in the observation space
      action_space:
        type: "Tuple"  # Custom type to indicate a combination of discrete and continuous spaces
        shape:
          discrete: [5]  # 5 discrete actions
          continuous: [3]  # 3 continuous actions

env_runners:
  num_env_runners: 1                # Use one environment runner
  num_envs_per_env_runner: 1        # One environment per runner (as you're running a single environment)
  num_cpus_per_env_runner: 1        # Assign one CPU per environment runner (you have only 4 CPUs available)
  num_gpus_per_env_runner: 0        # No GPUs per environment runner
  rollout_fragment_length: "auto"
  batch_mode: "truncate_episodes"

training:
  num_workers: 2                    # Allocate 2 worker processes for training (this is feasible with 4 CPUs)
  num_gpus: 0                       # No GPUs for training
  train_batch_size: 1024
  sgd_minibatch_size: 64
  num_epochs: 1
  lr: 3.0e-4
  gamma: 0.99
  lambda: 0.95
  clip_param: 0.2
  vf_clip_param: 10.0
  entropy_coeff: 0.01
  kl_coeff: 0.5
  vf_loss_coeff: 1.0

environment:
  disable_env_checking: false

stop:
  training_iteration: 10000
